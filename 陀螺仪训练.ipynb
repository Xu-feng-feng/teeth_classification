{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24466211",
   "metadata": {},
   "source": [
    "## Build and train neural networks--IMU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "from data_load import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a85f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"logs/scalars/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d0ee83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_function(data, label):\n",
    "  reshaped_data = tf.reshape(data, [-1, 3, 1])\n",
    "  return reshaped_data, label\n",
    "\n",
    "\n",
    "def calculate_model_size(model):\n",
    "  print(model.summary())\n",
    "  var_sizes = [\n",
    "      np.product(list(map(int, v.shape))) * v.dtype.size\n",
    "      for v in model.trainable_variables\n",
    "  ]\n",
    "  print(\"Model size:\", sum(var_sizes) / 1024, \"KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1972c5da",
   "metadata": {},
   "source": [
    "## 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638084b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(seq_length):\n",
    "  \"\"\"Builds a convolutional neural network in Keras.\"\"\"\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(\n",
    "          8, (4, 3),\n",
    "          padding=\"same\",\n",
    "          activation=\"relu\",\n",
    "          input_shape=(seq_length, 3, 1)),  # output_shape=(batch, 128, 3, 8)\n",
    "      tf.keras.layers.MaxPool2D((3, 3)),  # (batch, 42, 1, 8)\n",
    "      tf.keras.layers.Dropout(0.1),  # (batch, 42, 1, 8)\n",
    "      tf.keras.layers.Conv2D(16, (4, 1), padding=\"same\",\n",
    "                             activation=\"relu\"),  # (batch, 42, 1, 16)\n",
    "      tf.keras.layers.MaxPool2D((3, 1), padding=\"same\"),  # (batch, 14, 1, 16)\n",
    "      tf.keras.layers.Dropout(0.1),  # (batch, 14, 1, 16)\n",
    "      tf.keras.layers.Flatten(),  # (batch, 224)\n",
    "      tf.keras.layers.Dense(16, activation=\"relu\"),  # (batch, 16)\n",
    "      tf.keras.layers.Dropout(0.1),  # (batch, 16)\n",
    "      tf.keras.layers.Dense(3, activation=\"softmax\")  # (batch, 4)\n",
    "  ])\n",
    "  model_path = os.path.join(\"./netmodels\", \"CNN\")\n",
    "  print(\"Built CNN.\")\n",
    "  if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "  # model.load_weights(\"./netmodels/CNN/weights.h5\")\n",
    "  return model, model_path\n",
    "\n",
    "\n",
    "def build_lstm(seq_length):\n",
    "  \"\"\"Builds an LSTM in Keras.\"\"\"\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Bidirectional(\n",
    "          tf.keras.layers.LSTM(22),\n",
    "          input_shape=(seq_length, 3)),  # output_shape=(batch, 44)\n",
    "      tf.keras.layers.Dense(16, activation=\"sigmoid\")  # (batch, 4)\n",
    "  ])\n",
    "  model_path = os.path.join(\"./netmodels\", \"LSTM\")\n",
    "  print(\"Built LSTM.\")\n",
    "  if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "  return model, model_path\n",
    "\n",
    "def load_data(train_data_path, valid_data_path, test_data_path, seq_length):\n",
    "  data_loader = DataLoader(\n",
    "      train_data_path, valid_data_path, test_data_path, seq_length=seq_length)\n",
    "  data_loader.format()\n",
    "  return data_loader.train_len, data_loader.train_data, data_loader.valid_len, \\\n",
    "      data_loader.valid_data, data_loader.test_len, data_loader.test_data\n",
    "\n",
    "def build_net(args, seq_length):\n",
    "  if args.model == \"CNN\":\n",
    "    model, model_path = build_cnn(seq_length)\n",
    "  elif args.model == \"LSTM\":\n",
    "    model, model_path = build_lstm(seq_length)\n",
    "  else:\n",
    "    print(\"Please input correct model name.(CNN  LSTM)\")\n",
    "  return model, model_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc174df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(\n",
    "    model,\n",
    "    model_path,  # pylint: disable=unused-argument\n",
    "    train_len,  # pylint: disable=unused-argument\n",
    "    train_data,\n",
    "    valid_len,\n",
    "    valid_data,  # pylint: disable=unused-argument\n",
    "    test_len,\n",
    "    test_data,\n",
    "    kind):\n",
    "  \"\"\"Trains the model.\"\"\"\n",
    "  calculate_model_size(model)\n",
    "  epochs = 10     # epochs = 50\n",
    "  batch_size = 64\n",
    "  model.compile(\n",
    "      optimizer=\"adam\",\n",
    "      loss=\"sparse_categorical_crossentropy\",\n",
    "      metrics=[\"accuracy\"])\n",
    "  if kind == \"CNN\":\n",
    "    train_data = train_data.map(reshape_function)\n",
    "    test_data = test_data.map(reshape_function)\n",
    "    valid_data = valid_data.map(reshape_function)\n",
    "  test_labels = np.zeros(test_len)\n",
    "  idx = 0\n",
    "  for data, label in test_data:  # pylint: disable=unused-variable\n",
    "    test_labels[idx] = label.numpy()\n",
    "    idx += 1\n",
    "  train_data = train_data.batch(batch_size).repeat()\n",
    "  valid_data = valid_data.batch(batch_size)\n",
    "  test_data = test_data.batch(batch_size)\n",
    "  model.fit(\n",
    "      train_data,\n",
    "      epochs=epochs,\n",
    "      validation_data=valid_data,\n",
    "      steps_per_epoch=1000,\n",
    "      validation_steps=int((valid_len - 1) / batch_size + 1),\n",
    "      callbacks=[tensorboard_callback])\n",
    "  loss, acc = model.evaluate(test_data)\n",
    "  pred = np.argmax(model.predict(test_data), axis=1)\n",
    "  confusion = tf.math.confusion_matrix(\n",
    "      labels=tf.constant(test_labels),\n",
    "      predictions=tf.constant(pred),\n",
    "      num_classes=16)    # num_classes=4\n",
    "  print(confusion)\n",
    "  print(\"Loss {}, Accuracy {}\".format(loss, acc))\n",
    "  # Convert the model to the TensorFlow Lite format without quantization\n",
    "  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "  tflite_model = converter.convert()\n",
    "\n",
    "  # Save the model to disk\n",
    "  open(\"model.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "  # Convert the model to the TensorFlow Lite format with quantization\n",
    "  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "  converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "  tflite_model = converter.convert()\n",
    "\n",
    "  # Save the model to disk\n",
    "  open(\"model_quantized.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "  basic_model_size = os.path.getsize(\"model.tflite\")\n",
    "  print(\"Basic model is %d bytes\" % basic_model_size)\n",
    "  quantized_model_size = os.path.getsize(\"model_quantized.tflite\")\n",
    "  print(\"Quantized model is %d bytes\" % quantized_model_size)\n",
    "  difference = basic_model_size - quantized_model_size\n",
    "  print(\"Difference is %d bytes\" % difference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f438b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\"--model\", \"-m\")\n",
    "  parser.add_argument(\"--person\", \"-p\")\n",
    "  args = parser.parse_args()\n",
    "\n",
    "  seq_length = 128\n",
    "\n",
    "  print(\"Start to load data...\")\n",
    "  if args.person == \"true\":\n",
    "    train_len, train_data, valid_len, valid_data, test_len, test_data = \\\n",
    "        load_data(\"./person_split/train\", \"./person_split/valid\",\n",
    "                  \"./person_split/test\", seq_length)\n",
    "  else:\n",
    "    train_len, train_data, valid_len, valid_data, test_len, test_data = \\\n",
    "        load_data(\"./data/train\", \"./data/valid\", \"./data/test_zzc\", seq_length)\n",
    "\n",
    "  print(\"Start to build net...\")\n",
    "  model, model_path = build_net(args, seq_length)\n",
    "\n",
    "  print(\"Start training...\")\n",
    "  train_net(model, model_path, train_len, train_data, valid_len, valid_data,\n",
    "            test_len, test_data, args.model)\n",
    "\n",
    "  print(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ed3cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
